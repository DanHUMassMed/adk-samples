# RAG Agent with Google ADK, ChromaDB, and Phoenix

This project demonstrates a **Retrieval-Augmented Generation (RAG)** agent built using the **Google Agent Developer Kit (ADK)**. It uses **ChromaDB** as a vector store for document retrieval, **Ollama** (via LiteLLM) for the LLM, and **Arize Phoenix** for tracing and evaluation.

The agent allows users to ask questions about a specific PDF document (by default, Alphabet's 2024 Form 10-K), retrieves relevant chunks from the local vector database, and synthesizes an answer.

## ðŸ“‹ Prerequisites

Before running this project, ensure you have the following installed:

1. **Python 3.13+**
2. **[uv](https://github.com/astral-sh/uv)** (recommended for dependency management) or `pip`.
3. **[Ollama](https://ollama.com/)**: This project is configured to use a local LLM hosted by Ollama.
   * **Action Required:** Install Ollama and pull the model used in `rag_agent/agent.py` (default: `gpt-oss:20b` or similar).
   * *Note: Check `rag_agent/agent.py` for the exact model name. You can change this to any model you have locally, e.g., `llama3`.*

   ```bash
   ollama pull llama3  # Example if you switch the config
   ```

## ðŸš€ Installation

1. **Clone the repository** (if you haven't already).
2. **Install dependencies**:
   This project uses `uv` for package management.

   ```bash
   uv sync
   source .venv/bin/activate
   ```

   Or with pip:
   ```bash
   pip install .
   ```

---

## ðŸ“š Tutorial & User Guide

Follow these steps to go from a fresh checkout to a fully running RAG agent with observability.

### Step 1: Load the Data (Knowledge Base)

Before the agent can answer questions, we need to process our PDF document and store it in the vector database.

1. Navigate to the `notebooks/` directory.
2. Open and run **`load-data.ipynb`**.

**What this does:**

* Reads `data/alphabet-form-10-K-2024.pdf`.
* Splits the text into chunks using `RecursiveCharacterTextSplitter`.
* Embeds the text using `sentence-transformers/all-MiniLM-L6-v2`.
* Saves the vectors and text to a local ChromaDB instance in `chroma_db_chunks/`.

### Step 2: Start Observability (Phoenix)

Arize Phoenix provides real-time tracing and evaluation for your agent. Start the Phoenix server to visualize your agent's thinking process.

```bash
phoenix serve
```

* **Access the UI:** Open your browser to [http://0.0.0.0:6006/](http://0.0.0.0:6006/).
* Keep this terminal running in the background.

### Step 3: Run the Agent

Now, let's run the agent interactively using the ADK web interface. This starts a local web server where you can chat with your agent.

```bash
# Ensure you are in the project root (adk-samples/RAG)
adk web 
```

*(Note: If `adk` is not in your path, ensure your virtual environment is activated).*

* **Chat:** Open the provided local URL (usually `http://localhost:5173` or similar) and ask questions like:
  > "What are the risk factors mentioned in the 10-K?"
  >

The agent will:

1. Receive your query.
2. Call the `ask_chromadb` tool to search the vector store.
3. Generate an answer based on the retrieved documents.
4. Send trace data to Phoenix (check the Phoenix UI tabs!).

### Step 4: Run Tests & Evaluation

We use `pytest` for unit tests and a custom script for Phoenix-based evaluation.

**Run Unit Tests:**
Execute pytest from the root `RAG/` directory.

```bash
pytest
```

**Run Phoenix Evaluation:**
This script runs a set of test conversations (`eval/data/conversation.test.1.json`) against the agent and records the results in Phoenix.

```bash
python eval/test_eval_phoenix.py
```

**What happens:**

* The script simulates user queries.
* It captures the agent's response and tool usage.
* It runs evaluators (Exact Match, Tool Precision) to grade the performance.
* You can view these experiments in the **Phoenix UI** under the "Experiments" tab.

---

## ðŸ§  Codebase Deep Dive

### `rag_agent/`

* **`agent.py`**: Defines the `root_agent`. It configures the LLM (LiteLLM connecting to Ollama) and registers the tools. It also initializes Phoenix tracing.
* **`tools.py`**: Contains the `ask_chromadb` function. This is the bridge between the LLM and your data. It embeds the user's query and finds the nearest neighbors in the ChromaDB collection.
* **`tracing.py`**: Helper functions to hook into OpenInference/Phoenix for telemetry.

### `chroma_db_chunks/`

* This directory contains the SQLite database and binary files for ChromaDB. It is generated by the `load-data.ipynb` notebook. **Do not edit manually.**

### `eval/`

* **`test_eval_phoenix.py`**: An advanced evaluation script. It loads a dataset of questions and expected tool usages, runs the agent against them, and computes metrics like "Did the agent use the retrieval tool?" and "Did the tool name match?".
* **`data/`**: Contains JSON files defining the test cases (inputs and expected outputs).

### `notebooks/`

* **`load-data.ipynb`**: The ETL (Extract, Transform, Load) pipeline. It handles PDF parsing, text chunking, and vector embedding.

## ðŸ›  Troubleshooting

* **ChromaDB Collection Not Found:** Ensure you ran the `load-data.ipynb` notebook fully. The `chroma_db_chunks` folder must exist.
* **LLM Connection Error:** Ensure Ollama is running (`ollama serve`) and the model specified in `rag_agent/agent.py` is pulled (`ollama pull <model_name>`).
* **Phoenix Not Receiving Traces:** Ensure `phoenix serve` is running and the port `6006` is accessible.
