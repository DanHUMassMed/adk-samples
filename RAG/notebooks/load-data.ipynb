{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e0ad51",
   "metadata": {},
   "source": [
    "## Load a PDF into ChromaDB with Page-Level Source Metadata\n",
    "\n",
    "This cell ingests a PDF document (e.g., the Alphabet 10-K), splits it into **semantically meaningful text chunks**, and stores them in **ChromaDB** for retrieval-augmented generation (RAG).\n",
    "\n",
    "### What this does\n",
    "\n",
    "* Reads the PDF **page by page**\n",
    "* Splits each page into overlapping text chunks using `RecursiveCharacterTextSplitter`\n",
    "* Stores each chunk as a separate document in ChromaDB\n",
    "* Attaches **rich metadata** to every chunk so responses can be precisely cited\n",
    "\n",
    "### Metadata captured per chunk\n",
    "\n",
    "Each stored chunk includes:\n",
    "\n",
    "* **`source`** — original PDF filename\n",
    "* **`page_number`** — page in the PDF where the text originated\n",
    "* **`chunk_id`** — chunk index within the page\n",
    "* **`char_start` / `char_end`** — character offsets within the page text\n",
    "\n",
    "This enables downstream features such as:\n",
    "\n",
    "* Citations like *“Source: Alphabet_10K.pdf, page 42”*\n",
    "* Debugging exactly where retrieved text came from\n",
    "* More transparent and trustworthy RAG answers\n",
    "\n",
    "### Why page-level chunking?\n",
    "\n",
    "Chunking per page (instead of across the full document) preserves:\n",
    "\n",
    "* Accurate page references\n",
    "* Cleaner citations\n",
    "* Better alignment with how humans reference documents\n",
    "\n",
    "After running this cell, the document is ready to be queried by the RAG agent using ChromaDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59373a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/Code/Vibe_Coding/adk-samples/RAG/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from pypdf import PdfReader\n",
    "from chromadb.utils import embedding_functions\n",
    "# Import the specific text splitter class\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_pdf_into_chromadb(\n",
    "    file_path: str,\n",
    "    collection_name: str = \"alphabet_10k_collection_chunks\",\n",
    "    db_path: str = \"../chroma_db_chunks\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a PDF file, chunks its content per page using RecursiveCharacterTextSplitter,\n",
    "    and loads it into ChromaDB with rich source metadata.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    metadata_list = []\n",
    "    ids = []\n",
    "\n",
    "    for page_index, page in enumerate(reader.pages):\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        if not page_text:\n",
    "            continue\n",
    "\n",
    "        # Split THIS PAGE only\n",
    "        page_chunks = text_splitter.split_text(page_text)\n",
    "\n",
    "        char_cursor = 0\n",
    "        for chunk_index, chunk in enumerate(page_chunks):\n",
    "            char_start = page_text.find(chunk, char_cursor)\n",
    "            char_end = char_start + len(chunk)\n",
    "            char_cursor = char_end\n",
    "\n",
    "            documents.append(chunk)\n",
    "            metadata_list.append({\n",
    "                \"source\": os.path.basename(file_path),\n",
    "                \"page_number\": page_index, \n",
    "                \"chunk_id\": chunk_index + 1,\n",
    "                \"char_start\": char_start,\n",
    "                \"char_end\": char_end,\n",
    "            })\n",
    "\n",
    "            ids.append(\n",
    "                f\"{os.path.basename(file_path)}_p{page_index+1}_c{chunk_index+1}\"\n",
    "            )\n",
    "\n",
    "    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadata_list,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Successfully loaded {len(documents)} chunks into ChromaDB \"\n",
    "        f\"collection '{collection_name}' with page-level metadata.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1827dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 449 chunks into ChromaDB collection 'alphabet_10k_collection_chunks' with page-level metadata.\n"
     ]
    }
   ],
   "source": [
    "# Run the utility\n",
    "pdf_file_path = \"../data/alphabet-form-10-K-2024.pdf\"\n",
    "load_pdf_into_chromadb(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c73df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
